---
phase: 07-query-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - apps/convex/convex/lib/confidence.ts
  - apps/convex/convex/lib/prompt.ts
  - apps/convex/package.json
autonomous: true

must_haves:
  truths:
    - "Confidence score reflects retrieval quality, not LLM self-assessment"
    - "Prompt includes numbered chunks for citation tracking"
    - "Prompt instructs Claude to separate response by jurisdiction"
  artifacts:
    - path: "apps/convex/convex/lib/confidence.ts"
      provides: "Retrieval-based confidence scoring"
      exports: ["calculateConfidence"]
    - path: "apps/convex/convex/lib/prompt.ts"
      provides: "Claude prompt templates for RAG"
      exports: ["buildSystemPrompt", "buildUserPrompt", "SYSTEM_PROMPT"]
  key_links:
    - from: "apps/convex/convex/lib/confidence.ts"
      to: "RetrievedChunk metrics"
      via: "avgSimilarity, jurisdictionCoverage, citationCoverage"
      pattern: "chunks.reduce"
---

<objective>
Create confidence scoring and Claude prompt templates for answer generation.

Purpose: Confidence scoring (retrieval-based, not LLM self-assessment) helps lawyers understand answer reliability. Prompts instruct Claude to generate jurisdiction-layered responses with inline citations per CONTEXT.md decisions.

Output:
- Confidence scoring based on retrieval metrics (similarity, coverage)
- System and user prompt templates for Claude RAG
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-query-pipeline/07-CONTEXT.md
@.planning/phases/07-query-pipeline/07-RESEARCH.md
@.planning/phases/07-query-pipeline/07-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create confidence scoring module</name>
  <files>apps/convex/convex/lib/confidence.ts</files>
  <action>
Create confidence scoring module in apps/convex/convex/lib/confidence.ts:

1. **calculateConfidence function:**
   ```typescript
   export function calculateConfidence(
     chunks: RetrievedChunk[],
     targetJurisdictions: string[]
   ): ConfidenceScore
   ```

   Implementation:

   a. **Metric 1: Average Semantic Similarity**
   ```typescript
   const avgSimilarity = chunks.length > 0
     ? chunks.reduce((sum, c) => sum + c.score, 0) / chunks.length
     : 0;
   ```

   b. **Metric 2: Jurisdiction Coverage**
   ```typescript
   const representedJurisdictions = new Set(chunks.map(c => c.jurisdiction));
   const jurisdictionCoverage = targetJurisdictions.length > 0
     ? representedJurisdictions.size / targetJurisdictions.length
     : 0;
   ```

   c. **Metric 3: Citation Coverage (chunks with citations)**
   ```typescript
   const chunksWithCitations = chunks.filter(c => c.citation && c.citation.length > 0);
   const citationCoverage = chunks.length > 0
     ? chunksWithCitations.length / chunks.length
     : 0;
   ```

   d. **Combined Score (weighted)**
   ```typescript
   const score =
     avgSimilarity * 0.5 +      // 50% similarity
     jurisdictionCoverage * 0.3 + // 30% coverage
     citationCoverage * 0.2;    // 20% citation quality
   ```

   e. **Classification**
   ```typescript
   let level: 'High' | 'Medium' | 'Low';
   if (score > 0.8 && jurisdictionCoverage === 1.0) {
     level = 'High';
   } else if (score > 0.6) {
     level = 'Medium';
   } else {
     level = 'Low';
   }
   ```

   f. **Reason Generation**
   ```typescript
   const reason = `${level}: ${representedJurisdictions.size}/${targetJurisdictions.length} jurisdictions covered, avg similarity ${avgSimilarity.toFixed(2)}, ${(citationCoverage * 100).toFixed(0)}% chunks have citations`;
   ```

   g. **Return ConfidenceScore**
   ```typescript
   return {
     level,
     score,
     reason,
     metrics: {
       avgSimilarity,
       jurisdictionCoverage,
       citationCoverage
     }
   };
   ```

2. **Constants:**
   - SIMILARITY_WEIGHT = 0.5
   - COVERAGE_WEIGHT = 0.3
   - CITATION_WEIGHT = 0.2
   - HIGH_THRESHOLD = 0.8
   - MEDIUM_THRESHOLD = 0.6

Import RetrievedChunk, ConfidenceScore from "../query/types".

Note: This is retrieval-based confidence, NOT LLM self-assessment. The research is clear that LLMs overestimate confidence.
  </action>
  <verify>
```bash
cd apps/convex && npx tsc --noEmit
```
  </verify>
  <done>calculateConfidence returns score based on retrieval metrics (similarity, jurisdiction coverage, citation coverage)</done>
</task>

<task type="auto">
  <name>Task 2: Create Claude prompt templates</name>
  <files>apps/convex/convex/lib/prompt.ts</files>
  <action>
Create prompt template module in apps/convex/convex/lib/prompt.ts:

1. **SYSTEM_PROMPT constant:**
   ```typescript
   export const SYSTEM_PROMPT = `You are a legal compliance research assistant for ComplianceIQ. Your role is to answer regulatory compliance questions for lawyers using ONLY the provided regulatory text.

## Citation Rules
- Cite ALL factual claims using [N] format where N is the source number
- Example: "Food facilities must register with FDA [1] and maintain written food safety plans [2]."
- NEVER make claims without citations
- If information is not in the provided sources, state "Not found in available sources"

## Response Structure
Organize your response into sections by jurisdiction level:

### Federal
[Federal regulations that apply]

### State
[State statutes and administrative code that apply]

### County (if applicable)
[County ordinances that apply]

### Municipal (if applicable)
[City codes that apply]

### Required Permits and Licenses
For each permit/license identified:
- **Permit Name**: [name]
- **Issuing Agency**: [agency name]
- **Jurisdiction**: [federal/state/county/municipal]
- **Link**: [URL if available]
- **Regulatory Reference**: [citation]

## Important
- Be precise with citations - use exact section numbers from the metadata
- If regulations conflict or overlap, explain which takes precedence
- If coverage for a jurisdiction is incomplete, note: "Additional [jurisdiction] requirements may apply"
- Keep answers focused on the specific question asked`;
   ```

2. **buildSystemPrompt function:**
   ```typescript
   export function buildSystemPrompt(): string {
     return SYSTEM_PROMPT;
   }
   ```
   (Simple wrapper for consistency; may add customization later)

3. **buildUserPrompt function:**
   ```typescript
   export function buildUserPrompt(
     question: string,
     chunks: RetrievedChunk[],
     jurisdictions: string[]
   ): string
   ```

   Implementation:
   ```typescript
   // Number chunks for citation tracking
   const numberedChunks = chunks.map((chunk, i) => {
     const header = `[${i + 1}] ${chunk.citation} (${chunk.jurisdiction}, ${chunk.sourceType})`;
     const title = chunk.title ? `\nTitle: ${chunk.title}` : '';
     return `${header}${title}\n\n${chunk.text}`;
   }).join('\n\n---\n\n');

   // Build jurisdictions context
   const jurisdictionContext = jurisdictions.length > 0
     ? `\nRelevant jurisdictions: ${jurisdictions.join(', ')}`
     : '';

   return `Question: ${question}${jurisdictionContext}

## Regulatory Sources (cite using [N] format)

${numberedChunks}

## Instructions
Answer the question using ONLY the sources above. Organize by jurisdiction level (Federal, State, County, Municipal). List all required permits and licenses in a dedicated section at the end.`;
   ```

4. **buildCitationList function:**
   ```typescript
   export function buildCitationList(chunks: RetrievedChunk[]): string
   ```
   - Returns formatted citation list for reference display in UI/CLI output:
   ```
   [1] 21 C.F.R. ยง 117.5 (US, federal)
   [2] Tex. Health & Safety Code Ann. ยง 431.002 (TX, state)
   ...
   ```
   - Called by test-query.ts script to display citation references after answer
   - Will be used by frontend components in Phase 8 for citation footnotes

Import RetrievedChunk from "../query/types".
  </action>
  <verify>
```bash
cd apps/convex && npx tsc --noEmit
```
  </verify>
  <done>buildSystemPrompt returns RAG system instructions, buildUserPrompt formats question with numbered chunks</done>
</task>

</tasks>

<verification>
1. apps/convex/convex/lib/confidence.ts exists with calculateConfidence
2. apps/convex/convex/lib/prompt.ts exists with SYSTEM_PROMPT, buildSystemPrompt, buildUserPrompt, buildCitationList
3. TypeScript compiles: `cd apps/convex && npx tsc --noEmit`
4. Prompts include jurisdiction sections (Federal, State, County, Municipal) per CONTEXT.md
5. Confidence uses retrieval metrics, not LLM self-assessment
</verification>

<success_criteria>
- calculateConfidence returns High/Medium/Low based on similarity, coverage, citation metrics
- System prompt instructs Claude to cite [N] and organize by jurisdiction
- User prompt numbers chunks [1], [2], [3] for citation tracking
- Prompt includes dedicated "Required Permits" section instruction
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/07-query-pipeline/07-03-SUMMARY.md`
</output>
