---
phase: 02-federal-data
plan: 05
type: execute
wave: 3
depends_on: ["02-04"]
files_modified:
  - apps/workers/src/federal/embed.ts
autonomous: true

user_setup:
  - service: openai
    why: "Embedding generation via text-embedding-3-large"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API Keys -> Create new secret key"

must_haves:
  truths:
    - "Embeddings generated for all chunks via OpenAI API"
    - "Batch processing respects rate limits"
    - "Token validation prevents API errors"
    - "Failed embeddings handled with retry logic"
  artifacts:
    - path: "apps/workers/src/federal/embed.ts"
      provides: "OpenAI embedding generation"
      exports: ["generateEmbeddings", "embedChunks"]
  key_links:
    - from: "apps/workers/src/federal/embed.ts"
      to: "apps/workers/src/lib/tokens.ts"
      via: "validateChunkSize before API call"
      pattern: "validateChunkSize"
    - from: "apps/workers/src/federal/embed.ts"
      to: "openai"
      via: "embeddings.create API"
      pattern: "openai\\.embeddings\\.create"
---

<objective>
Create embedding pipeline using OpenAI text-embedding-3-large.

Purpose: Generate vector embeddings for CFR chunks. The embeddings enable semantic search in Pinecone. Must handle batching, rate limits, and token validation to avoid API errors.

Output:
- Embedding generator with batching (64 chunks per request)
- Token validation before API calls
- Retry logic for rate limit handling
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-federal-data/02-CONTEXT.md
@.planning/phases/02-federal-data/02-RESEARCH.md
@.planning/phases/02-federal-data/02-04-SUMMARY.md
@apps/workers/src/federal/types.ts
@apps/workers/src/federal/chunk.ts
@apps/workers/src/lib/tokens.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install OpenAI SDK and create embedding generator</name>
  <files>
    apps/workers/package.json
    apps/workers/src/federal/embed.ts
  </files>
  <action>
1. Install OpenAI SDK:
   ```bash
   pnpm add --filter @compliance-iq/workers openai
   ```

2. Create `apps/workers/src/federal/embed.ts`:

3. `generateEmbeddings(texts: string[], apiKey: string): Promise<number[][]>`
   - Input: Array of text strings to embed
   - Output: Array of 3072-dimension vectors (one per text)

   Implementation:
   ```typescript
   import OpenAI from 'openai';

   const openai = new OpenAI({ apiKey });

   const response = await openai.embeddings.create({
     model: 'text-embedding-3-large',
     input: texts,
     encoding_format: 'float',
   });

   return response.data.map(d => d.embedding);
   ```

4. Token validation before API call:
   - Use validateChunkSize() from lib/tokens.ts
   - Reject any text > 8191 tokens (hard limit for text-embedding-3-large)
   - Throw descriptive error if validation fails

5. Rate limit handling:
   - Catch 429 errors (rate limit exceeded)
   - Implement exponential backoff: 1s, 2s, 4s, 8s (max 4 retries)
   - Log retry attempts

6. Error types:
   ```typescript
   class EmbeddingError extends Error {
     constructor(
       message: string,
       public readonly code: 'RATE_LIMIT' | 'TOKEN_LIMIT' | 'API_ERROR',
       public readonly details?: unknown
     ) {
       super(message);
     }
   }
   ```
  </action>
  <verify>
    - pnpm install completes without errors
    - pnpm --filter @compliance-iq/workers exec tsc --noEmit passes
    - generateEmbeddings function exported
  </verify>
  <done>
    - OpenAI SDK installed
    - Embedding generator with 3072-dim vectors
    - Token validation prevents oversized requests
    - Retry logic for rate limits
  </done>
</task>

<task type="auto">
  <name>Task 2: Create batch embedding processor for chunks</name>
  <files>apps/workers/src/federal/embed.ts</files>
  <action>
Add batch processing to `apps/workers/src/federal/embed.ts`:

1. `embedChunks(chunks: CFRChunk[], apiKey: string): Promise<EmbeddedChunk[]>`
   - Input: Array of CFRChunk objects
   - Output: Array of EmbeddedChunk (chunk + embedding vector)

   ```typescript
   interface EmbeddedChunk {
     chunk: CFRChunk;
     embedding: number[];  // 3072-dim vector
   }
   ```

2. Batch processing:
   - Process in batches of 64 chunks (recommended for text-embedding-3-large)
   - Add 100ms delay between batches to avoid rate limits
   - Log progress: "Embedding batch {current}/{total}"

   Implementation:
   ```typescript
   const BATCH_SIZE = 64;
   const BATCH_DELAY_MS = 100;
   const results: EmbeddedChunk[] = [];

   for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
     const batch = chunks.slice(i, i + BATCH_SIZE);
     const texts = batch.map(c => c.text);

     // Validate all texts in batch
     for (const chunk of batch) {
       const validation = validateChunkSize(chunk.text, 8191);
       if (!validation.valid) {
         throw new EmbeddingError(
           `Chunk ${chunk.chunkId} exceeds token limit: ${validation.tokens}`,
           'TOKEN_LIMIT'
         );
       }
     }

     const embeddings = await generateEmbeddings(texts, apiKey);

     batch.forEach((chunk, idx) => {
       results.push({ chunk, embedding: embeddings[idx] });
     });

     // Progress logging
     console.log(`Embedding batch ${Math.floor(i / BATCH_SIZE) + 1}/${Math.ceil(chunks.length / BATCH_SIZE)}`);

     // Delay between batches
     if (i + BATCH_SIZE < chunks.length) {
       await new Promise(resolve => setTimeout(resolve, BATCH_DELAY_MS));
     }
   }

   return results;
   ```

3. Statistics:
   ```typescript
   interface EmbeddingStats {
     totalChunks: number;
     totalBatches: number;
     avgEmbeddingTimeMs: number;
     failedChunks: number;
   }
   ```

4. Progress callback (optional):
   ```typescript
   type ProgressCallback = (current: number, total: number) => void;

   function embedChunks(
     chunks: CFRChunk[],
     apiKey: string,
     onProgress?: ProgressCallback
   ): Promise<EmbeddedChunk[]>
   ```
  </action>
  <verify>
    - pnpm --filter @compliance-iq/workers exec tsc --noEmit passes
    - embedChunks processes chunks in batches of 64
    - Token validation runs before each batch
    - Progress logging works
  </verify>
  <done>
    - Batch processing with 64-chunk batches
    - 100ms delay prevents rate limit issues
    - Progress logging for monitoring
    - All chunks validated before embedding
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `pnpm --filter @compliance-iq/workers exec tsc --noEmit`
2. OpenAI SDK imported and configured
3. Embeddings are 3072-dimensional vectors
4. Batch processing respects rate limits
5. Token validation prevents API errors
</verification>

<success_criteria>
- OpenAI SDK installed and configured
- text-embedding-3-large generates 3072-dim vectors
- Batching in groups of 64 with delays
- Token validation catches oversized chunks
- Retry logic handles rate limits
</success_criteria>

<output>
After completion, create `.planning/phases/02-federal-data/02-05-SUMMARY.md`
</output>
