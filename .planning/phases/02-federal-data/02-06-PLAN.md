---
phase: 02-federal-data
plan: 06
type: execute
wave: 4
depends_on: ["02-02", "02-03", "02-05"]
files_modified:
  - apps/workers/src/federal/pipeline.ts
  - apps/workers/src/federal/index.ts
  - apps/workers/src/index.ts
autonomous: true

user_setup:
  - service: pinecone
    why: "Vector storage and search"
    env_vars:
      - name: PINECONE_API_KEY
        source: "Pinecone Console -> API Keys"
  - service: convex
    why: "Source freshness tracking"
    env_vars:
      - name: CONVEX_URL
        source: "Convex Dashboard -> Settings -> Deployment URL"

must_haves:
  truths:
    - "Pipeline fetches, stores, chunks, embeds, and indexes CFR titles"
    - "Vectors searchable in Pinecone with jurisdiction filter"
    - "Convex sources table updated with freshness"
    - "Pipeline can be triggered manually via HTTP endpoint"
  artifacts:
    - path: "apps/workers/src/federal/pipeline.ts"
      provides: "End-to-end federal data pipeline"
      exports: ["processCFRTitle", "processAllFederalTitles"]
    - path: "apps/workers/src/federal/index.ts"
      provides: "Federal module exports"
      exports: ["processCFRTitle", "processAllFederalTitles", "TARGET_TITLES"]
    - path: "apps/workers/src/index.ts"
      provides: "Pipeline trigger endpoint"
      contains: "/pipeline/federal"
  key_links:
    - from: "apps/workers/src/federal/pipeline.ts"
      to: "apps/workers/src/pinecone.ts"
      via: "upsertChunks for vector storage"
      pattern: "upsertChunks"
    - from: "apps/workers/src/federal/pipeline.ts"
      to: "apps/convex/convex/sources.ts"
      via: "HTTP call to Convex mutation"
      pattern: "CONVEX_URL"
---

<objective>
Create the complete federal data pipeline orchestrator and HTTP trigger.

Purpose: Orchestrate the full pipeline: fetch CFR titles from eCFR, store raw XML in R2, chunk sections, generate embeddings, upsert to Pinecone, and update Convex sources. Add HTTP endpoint for manual pipeline triggering.

Output:
- Pipeline orchestrator that processes CFR titles end-to-end
- Pinecone indexing with metadata for jurisdiction filtering
- Convex freshness sync
- HTTP endpoint for manual triggering
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-federal-data/02-CONTEXT.md
@.planning/phases/02-federal-data/02-RESEARCH.md
@.planning/phases/02-federal-data/02-02-SUMMARY.md
@.planning/phases/02-federal-data/02-03-SUMMARY.md
@.planning/phases/02-federal-data/02-05-SUMMARY.md
@apps/workers/src/pinecone.ts
@apps/workers/src/federal/types.ts
@apps/workers/src/federal/fetch.ts
@apps/workers/src/federal/storage.ts
@apps/workers/src/federal/chunk.ts
@apps/workers/src/federal/embed.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pipeline orchestrator</name>
  <files>apps/workers/src/federal/pipeline.ts</files>
  <action>
Create `apps/workers/src/federal/pipeline.ts` with end-to-end pipeline:

1. `processCFRTitle(titleNumber: number, env: Env): Promise<PipelineResult>`
   - Input: CFR title number (e.g., 21)
   - Output: Pipeline result with stats

   ```typescript
   interface PipelineResult {
     titleNumber: number;
     success: boolean;
     partsProcessed: number;
     chunksCreated: number;
     vectorsUpserted: number;
     durationMs: number;
     errors?: string[];
   }
   ```

   Pipeline steps:
   a) Load checkpoint from R2 (if exists, resume from last position)
   b) Fetch CFR title XML from eCFR API
   c) Parse XML into parts
   d) For each part not in checkpoint:
      - Store raw XML in R2
      - Chunk sections
      - Generate embeddings
      - Upsert to Pinecone
      - Update checkpoint
   e) Clear checkpoint on completion
   f) Return stats

2. Pinecone upsert format:
   ```typescript
   const records = embeddedChunks.map(({ chunk, embedding }) => ({
     id: chunk.chunkId,
     values: embedding,
     metadata: {
       chunkId: chunk.chunkId,
       sourceId: chunk.sourceId,
       sourceType: 'federal' as const,
       jurisdiction: 'US',
       text: chunk.text,
       citation: chunk.citation,
       title: chunk.title,
       chunkIndex: chunk.chunkIndex,
       totalChunks: chunk.totalChunks,
       category: chunk.category,
       indexedAt: new Date().toISOString(),
     },
   }));
   ```

3. Error handling:
   - Catch errors per part, continue with next part
   - Accumulate errors in result
   - Save checkpoint after each successful part (enables resume)

4. Logging:
   - Log start: "Processing CFR Title {number}"
   - Log per part: "Part {number}: {chunks} chunks, {vectors} vectors"
   - Log completion: "Title {number} complete: {total} vectors in {duration}ms"
  </action>
  <verify>
    - pnpm --filter @compliance-iq/workers exec tsc --noEmit passes
    - processCFRTitle orchestrates all pipeline steps
    - Checkpointing enables resume after failure
  </verify>
  <done>
    - End-to-end pipeline for single CFR title
    - R2 storage, chunking, embedding, Pinecone upsert
    - Checkpoint-based resume capability
    - Comprehensive logging
  </done>
</task>

<task type="auto">
  <name>Task 2: Add batch processor and Convex sync</name>
  <files>apps/workers/src/federal/pipeline.ts</files>
  <action>
Add batch processing and Convex sync to `apps/workers/src/federal/pipeline.ts`:

1. `processAllFederalTitles(env: Env): Promise<BatchPipelineResult>`
   - Process all 7 target CFR titles (from TARGET_TITLES)
   - Return aggregated results

   ```typescript
   interface BatchPipelineResult {
     success: boolean;
     titlesProcessed: number;
     totalChunks: number;
     totalVectors: number;
     durationMs: number;
     results: PipelineResult[];
   }
   ```

   Implementation:
   - Process titles sequentially (avoid overwhelming APIs)
   - Continue on individual title failure
   - Aggregate stats

2. `syncConvexSources(env: Env, results: BatchPipelineResult): Promise<void>`
   - Update Convex sources table with freshness data
   - Call Convex HTTP API (not SDK - Workers don't support full SDK)

   Implementation:
   ```typescript
   // Create or update federal source record
   const response = await fetch(`${env.CONVEX_URL}/api/mutation`, {
     method: 'POST',
     headers: {
       'Content-Type': 'application/json',
     },
     body: JSON.stringify({
       path: 'sources:updateStatus',
       args: {
         id: federalSourceId,  // Lookup or create
         status: results.success ? 'complete' : 'error',
         lastScrapedAt: Date.now(),
       },
     }),
   });
   ```

   Note: For MVP, we'll call the Convex HTTP API directly. This is simpler than setting up the full Convex client in Workers.

3. Error handling for Convex sync:
   - Log warning if sync fails (don't fail pipeline)
   - Record sync status in result
  </action>
  <verify>
    - pnpm --filter @compliance-iq/workers exec tsc --noEmit passes
    - processAllFederalTitles processes all 7 target titles
    - Convex sync updates sources table
  </verify>
  <done>
    - Batch processing for all 7 federal titles
    - Convex sources table updated with freshness
    - Pipeline continues on individual failures
  </done>
</task>

<task type="auto">
  <name>Task 3: Add HTTP trigger endpoint and module exports</name>
  <files>
    apps/workers/src/federal/index.ts
    apps/workers/src/index.ts
  </files>
  <action>
1. Create `apps/workers/src/federal/index.ts` (barrel export):
   ```typescript
   export { TARGET_TITLES, type CFRTitleConfig, type CFRChunk } from './types';
   export { fetchCFRTitle, fetchCFRPart, parseCFRXML } from './fetch';
   export { storeCFRPart, saveCheckpoint, loadCheckpoint } from './storage';
   export { chunkCFRSection, chunkCFRPart } from './chunk';
   export { embedChunks, generateEmbeddings } from './embed';
   export { processCFRTitle, processAllFederalTitles } from './pipeline';
   ```

2. Update `apps/workers/src/index.ts` with pipeline trigger:

   Add route handler for `/pipeline/federal`:
   ```typescript
   // POST /pipeline/federal - Trigger full federal data pipeline
   if (url.pathname === '/pipeline/federal' && request.method === 'POST') {
     try {
       const result = await processAllFederalTitles(env);
       return new Response(JSON.stringify(result), {
         headers: { 'Content-Type': 'application/json' },
       });
     } catch (error) {
       return new Response(
         JSON.stringify({
           error: 'Pipeline failed',
           message: error instanceof Error ? error.message : 'Unknown error',
         }),
         {
           status: 500,
           headers: { 'Content-Type': 'application/json' },
         }
       );
     }
   }

   // POST /pipeline/federal/:title - Trigger single title pipeline
   const titleMatch = url.pathname.match(/^\/pipeline\/federal\/(\d+)$/);
   if (titleMatch && request.method === 'POST') {
     const titleNumber = parseInt(titleMatch[1], 10);
     try {
       const result = await processCFRTitle(titleNumber, env);
       return new Response(JSON.stringify(result), {
         headers: { 'Content-Type': 'application/json' },
       });
     } catch (error) {
       return new Response(
         JSON.stringify({
           error: 'Pipeline failed',
           message: error instanceof Error ? error.message : 'Unknown error',
         }),
         {
           status: 500,
           headers: { 'Content-Type': 'application/json' },
         }
       );
     }
   }
   ```

3. Import pipeline at top of index.ts:
   ```typescript
   import { processCFRTitle, processAllFederalTitles } from './federal';
   ```

4. Document endpoints in health response:
   - /health - Health check
   - /documents - List R2 documents
   - POST /pipeline/federal - Trigger full federal pipeline
   - POST /pipeline/federal/:title - Trigger single title pipeline
  </action>
  <verify>
    - pnpm --filter @compliance-iq/workers exec tsc --noEmit passes
    - Federal module exports all functions
    - Pipeline endpoints added to worker
  </verify>
  <done>
    - Federal module exports consolidated
    - HTTP endpoints for manual pipeline triggering
    - Full pipeline and single-title modes available
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `pnpm --filter @compliance-iq/workers exec tsc --noEmit`
2. Worker starts: `pnpm --filter @compliance-iq/workers dev`
3. Health endpoint includes pipeline routes
4. Federal module exports all public functions
5. Pipeline orchestrates fetch -> store -> chunk -> embed -> index flow
</verification>

<success_criteria>
- End-to-end pipeline processes CFR titles
- Raw XML stored in R2 for audit trail
- Vectors indexed in Pinecone with jurisdiction filter
- Convex sources updated with freshness
- HTTP endpoints for manual triggering
- Checkpointing enables resume after failure
</success_criteria>

<output>
After completion, create `.planning/phases/02-federal-data/02-06-SUMMARY.md`
</output>
